{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Classification Model\n",
    "\n",
    "### Labeling text message data\n",
    "\n",
    "### Training a classifier on labeled text message data\n",
    "\n",
    "### Using the classifier to predict the urgency of tweet data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishan\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('datasets/disaster_response_messages_training.csv')\n",
    "test = pd.read_csv('datasets/disaster_response_messages_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21046, 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'split', 'message', 'original', 'genre', 'related', 'PII',\n",
       "       'request', 'offer', 'aid_related', 'medical_help', 'medical_products',\n",
       "       'search_and_rescue', 'security', 'military', 'child_alone', 'water',\n",
       "       'food', 'shelter', 'clothing', 'money', 'missing_people', 'refugees',\n",
       "       'death', 'other_aid', 'infrastructure_related', 'transport',\n",
       "       'buildings', 'electricity', 'tools', 'hospitals', 'shops',\n",
       "       'aid_centers', 'other_infrastructure', 'weather_related', 'floods',\n",
       "       'storm', 'fire', 'earthquake', 'cold', 'other_weather',\n",
       "       'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking column distributions\n",
    "\n",
    "# for i in train.columns[4:]:\n",
    "    #print(i)\n",
    "    #print(train[i].value_counts(normalize = True))\n",
    "    #print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with heavily skewed distributions\n",
    "\n",
    "train = train.drop(columns = ['id', 'split', 'original', 'tools', 'child_alone', 'PII'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['request', 'offer', 'aid_related', 'medical_help', 'medical_products',\n",
       "       'search_and_rescue', 'security', 'military', 'water', 'food', 'shelter',\n",
       "       'clothing', 'money', 'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns[3:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a target variable\n",
    "\n",
    "Based on the presence of certain features and a message being direct (not news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21046\n"
     ]
    }
   ],
   "source": [
    "need_help = []\n",
    "for row in range(train.shape[0]):\n",
    "    val = 0\n",
    "    for i in train.columns[3:35]:\n",
    "        if train[i][row] == 1 and train['genre'][row] == 'direct':\n",
    "            val = 1\n",
    "    need_help.append(val)\n",
    "print(len(need_help))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['need_help'] = need_help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.804761\n",
       "1    0.195239\n",
       "Name: need_help, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['need_help'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating model through example messages\n",
    "\n",
    "Messages categorized as urgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707     We need food, water, medicines. Thank you. We ...\n",
       "8604    canned and fresh food ( apples and veggies ) d...\n",
       "4360    We are in Carrefour Airport streets every day,...\n",
       "5408    HI,I NEVER FIND ANYTHING TO SURVIVE WITH MY SE...\n",
       "8729    I have baby blankets , diapers , formula , war...\n",
       "2755    Is it possible to know how many people died du...\n",
       "5795    they gave me some tablets which can treat wate...\n",
       "1219    Im Haitian and i've lost everything i own. I d...\n",
       "3940    On Dumas we are hungry - in the Croix des Bouq...\n",
       "2542    We are tired of sending messages. They do not ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['need_help'] == 1].sample(10)['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we live in Fontamara 27. We have problems with lack of food and shelter. please help us'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['need_help'] == 1]['message'][527]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am asking for your help please, because we can't no longer take this. Come rescue us, we can't find any help. Just stop by and come see us. I am waiting and I thamk you in advance\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['need_help'] == 1]['message'][540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are located in the first section of PetitBois, commune of Croix des Bouquets, we are in needs of food,tents in Dume, waiting for your help. '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['need_help'] == 1]['message'][3814]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have jackets , hoodies , baby formula , baby bottles and new nipples for bottles , rice cereal for babies , toys for babies and toddlers , towels , baby clothes and dried food'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['need_help'] == 1]['message'][9053]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message processing\n",
    "\n",
    "- Tokenizing\n",
    "- Lemmatizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating functions for tokenizing and lemmatizing\n",
    "\n",
    "# Processing message data\n",
    "\n",
    "def tokenize(x):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return tokenizer.tokenize(x)\n",
    "\n",
    "train['tokens'] = train['message'].map(tokenize)\n",
    "    \n",
    "def lemmatize(x):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "train['lemma'] = train['tokens'].map(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>...</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "      <th>need_help</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10609</td>\n",
       "      <td>According to the BNGRC's Soa, \"Ivan passed tho...</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[According, to, the, BNGRC, s, Soa, Ivan, pass...</td>\n",
       "      <td>According to the BNGRC s Soa Ivan passed thoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17342</td>\n",
       "      <td>Our own domestic disaster response model can s...</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Our, own, domestic, disaster, response, model...</td>\n",
       "      <td>Our own domestic disaster response model can s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>From where I am lying, I cannot walk to Delmas...</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[From, where, I, am, lying, I, cannot, walk, t...</td>\n",
       "      <td>From where I am lying I cannot walk to Delmas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12459</td>\n",
       "      <td>In this Republic, the rice and potato crop vir...</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[In, this, Republic, the, rice, and, potato, c...</td>\n",
       "      <td>In this Republic the rice and potato crop virt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10143</td>\n",
       "      <td>People braving the early stages of the storm. ...</td>\n",
       "      <td>social</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[People, braving, the, early, stages, of, the,...</td>\n",
       "      <td>People braving the early stage of the storm hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6321</td>\n",
       "      <td>Cyclon! but we don't have, we are in trouble</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Cyclon, but, we, don, t, have, we, are, in, t...</td>\n",
       "      <td>Cyclon but we don t have we are in trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11754</td>\n",
       "      <td>The association has set up a service consistin...</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[The, association, has, set, up, a, service, c...</td>\n",
       "      <td>The association ha set up a service consisting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5710</td>\n",
       "      <td>iv, santo 6 I would like us me messenger the g...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[iv, santo, 6, I, would, like, us, me, messeng...</td>\n",
       "      <td>iv santo 6 I would like u me messenger the goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2215</td>\n",
       "      <td>We are almost dead at Impasse Mousin ( mouzen ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[We, are, almost, dead, at, Impasse, Mousin, m...</td>\n",
       "      <td>We are almost dead at Impasse Mousin mouzen of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14957</td>\n",
       "      <td>Tractors and Oxen: The use of tractors continu...</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Tractors, and, Oxen, The, use, of, tractors, ...</td>\n",
       "      <td>Tractors and Oxen The use of tractor continues...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 message   genre  related  \\\n",
       "10609  According to the BNGRC's Soa, \"Ivan passed tho...    news        1   \n",
       "17342  Our own domestic disaster response model can s...    news        0   \n",
       "2725   From where I am lying, I cannot walk to Delmas...  direct        0   \n",
       "12459  In this Republic, the rice and potato crop vir...    news        1   \n",
       "10143  People braving the early stages of the storm. ...  social        1   \n",
       "6321       Cyclon! but we don't have, we are in trouble   direct        0   \n",
       "11754  The association has set up a service consistin...    news        1   \n",
       "5710   iv, santo 6 I would like us me messenger the g...  direct        1   \n",
       "2215   We are almost dead at Impasse Mousin ( mouzen ...  direct        1   \n",
       "14957  Tractors and Oxen: The use of tractors continu...    news        1   \n",
       "\n",
       "       request  offer  aid_related  medical_help  medical_products  \\\n",
       "10609        0      0            0             0                 0   \n",
       "17342        0      0            0             0                 0   \n",
       "2725         0      0            0             0                 0   \n",
       "12459        0      0            0             0                 0   \n",
       "10143        0      0            0             0                 0   \n",
       "6321         0      0            0             0                 0   \n",
       "11754        0      0            1             0                 0   \n",
       "5710         1      0            1             0                 0   \n",
       "2215         1      0            1             0                 0   \n",
       "14957        0      0            0             0                 0   \n",
       "\n",
       "       search_and_rescue  security  ...  floods  storm  fire  earthquake  \\\n",
       "10609                  0         0  ...       0      1     0           0   \n",
       "17342                  0         0  ...       0      0     0           0   \n",
       "2725                   0         0  ...       0      0     0           0   \n",
       "12459                  0         0  ...       0      0     0           0   \n",
       "10143                  0         0  ...       0      1     0           0   \n",
       "6321                   0         0  ...       0      0     0           0   \n",
       "11754                  0         0  ...       0      0     0           0   \n",
       "5710                   0         0  ...       0      0     0           0   \n",
       "2215                   0         0  ...       0      0     0           0   \n",
       "14957                  0         0  ...       0      0     0           0   \n",
       "\n",
       "       cold  other_weather  direct_report  need_help  \\\n",
       "10609     0              0              0          0   \n",
       "17342     0              0              0          0   \n",
       "2725      0              0              0          0   \n",
       "12459     0              0              0          0   \n",
       "10143     0              0              1          0   \n",
       "6321      0              0              0          0   \n",
       "11754     0              0              0          0   \n",
       "5710      0              0              1          1   \n",
       "2215      0              0              1          1   \n",
       "14957     0              0              0          0   \n",
       "\n",
       "                                                  tokens  \\\n",
       "10609  [According, to, the, BNGRC, s, Soa, Ivan, pass...   \n",
       "17342  [Our, own, domestic, disaster, response, model...   \n",
       "2725   [From, where, I, am, lying, I, cannot, walk, t...   \n",
       "12459  [In, this, Republic, the, rice, and, potato, c...   \n",
       "10143  [People, braving, the, early, stages, of, the,...   \n",
       "6321   [Cyclon, but, we, don, t, have, we, are, in, t...   \n",
       "11754  [The, association, has, set, up, a, service, c...   \n",
       "5710   [iv, santo, 6, I, would, like, us, me, messeng...   \n",
       "2215   [We, are, almost, dead, at, Impasse, Mousin, m...   \n",
       "14957  [Tractors, and, Oxen, The, use, of, tractors, ...   \n",
       "\n",
       "                                                   lemma  \n",
       "10609  According to the BNGRC s Soa Ivan passed thoug...  \n",
       "17342  Our own domestic disaster response model can s...  \n",
       "2725   From where I am lying I cannot walk to Delmas ...  \n",
       "12459  In this Republic the rice and potato crop virt...  \n",
       "10143  People braving the early stage of the storm hu...  \n",
       "6321          Cyclon but we don t have we are in trouble  \n",
       "11754  The association ha set up a service consisting...  \n",
       "5710   iv santo 6 I would like u me messenger the goo...  \n",
       "2215   We are almost dead at Impasse Mousin mouzen of...  \n",
       "14957  Tractors and Oxen The use of tractor continues...  \n",
       "\n",
       "[10 rows x 39 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing before modelling (Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21046x215177 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 524279 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
    "tf.fit_transform(train['lemma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Select model and gridsearch parameters according to preference\n",
    "\n",
    "Random forest, logistic regression, naive bayes are given in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y for training data (split already done)\n",
    "\n",
    "X = train['lemma']\n",
    "y = train['need_help']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest gridsearch\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "pgrid_rf = {\n",
    "    'tf__max_features' : [2000, 3000, 5000],\n",
    "    'tf__stop_words' : ['english', None],\n",
    "    'tf__ngram_range' : [(1,2)],\n",
    "    'tf__use_idf' : [True, False],\n",
    "    'rf__n_estimators' : [5, 10, 35],\n",
    "    'rf__max_depth' : [4, 5, 6],\n",
    "    'rf__max_features' : [None, 3, 6]\n",
    "}\n",
    "\n",
    "pipe_rf = Pipeline(steps = [('tf', TfidfVectorizer()), ('rf', RandomForestClassifier())])\n",
    "\n",
    "gs_rf = GridSearchCV(pipe_rf, pgrid_rf, cv = 5, n_jobs = -1, verbose=1)\n",
    "\n",
    "gs_rf.fit(X, y)\n",
    "\n",
    "gs_rf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>home</td>\n",
       "      <td>50.185096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>need help</td>\n",
       "      <td>29.974784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>near</td>\n",
       "      <td>27.524558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>important</td>\n",
       "      <td>23.202539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>national</td>\n",
       "      <td>22.495263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>damage</td>\n",
       "      <td>20.193607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>province</td>\n",
       "      <td>12.800056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>according</td>\n",
       "      <td>12.768996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>flood</td>\n",
       "      <td>11.908526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>condition</td>\n",
       "      <td>10.770423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                coef\n",
       "home       50.185096\n",
       "need help  29.974784\n",
       "near       27.524558\n",
       "important  23.202539\n",
       "national   22.495263\n",
       "damage     20.193607\n",
       "province   12.800056\n",
       "according  12.768996\n",
       "flood      11.908526\n",
       "condition  10.770423"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=.01)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "X_lr = tf.fit_transform(X)\n",
    "\n",
    "lr.fit(X_lr, y)\n",
    "\n",
    "# Creating a dataframe from exponentiated coeficients and vectorizer vocabulary\n",
    "\n",
    "\n",
    "coefs = pd.DataFrame(np.exp(lr.coef_), columns=tf.vocabulary_).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "coefs.columns = ['coef']\n",
    "\n",
    "coefs.sort_values(by = 'coef', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ishan\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3057: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv('datasets/df_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>8.96827e+17</td>\n",
       "      <td>Gert could become a quite intense post-tropica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>8.97013e+17</td>\n",
       "      <td>Weather Street: Tropical Storm Harvey, Hurrica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>8.97087e+17</td>\n",
       "      <td>Tropical Storm #Gert intensifying. Tropical St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>8.97088e+17</td>\n",
       "      <td>Tropical Storm #Gert intensifying. Tropical St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2017-08-14</td>\n",
       "      <td>8.97088e+17</td>\n",
       "      <td>RT YourNews15 \"Tropical Storm #Gert intensifyi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date           id                                              tweet\n",
       "0  2017-08-13  8.96827e+17  Gert could become a quite intense post-tropica...\n",
       "1  2017-08-14  8.97013e+17  Weather Street: Tropical Storm Harvey, Hurrica...\n",
       "2  2017-08-14  8.97087e+17  Tropical Storm #Gert intensifying. Tropical St...\n",
       "3  2017-08-14  8.97088e+17  Tropical Storm #Gert intensifying. Tropical St...\n",
       "4  2017-08-14  8.97088e+17  RT YourNews15 \"Tropical Storm #Gert intensifyi..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tokens'] = tweets['tweet'].map(tokenize)\n",
    "    \n",
    "tweets['lemma'] = tweets['tokens'].map(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting based on tweet data\n",
    "\n",
    "#### Random forest predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tweets = tweets['lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gs_rf.predict(X_tweets)\n",
    "\n",
    "tweets['need_help'] = preds\n",
    "\n",
    "tweets['need_help'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets['need_help'] == 1].sample(10)['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[['need_help']].to_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 360 out of 360 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=N...\n",
       "                                       ('nb',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'nb__alpha': [0.1, 0.5, 1],\n",
       "                         'tf__max_features': [2000, 3000, 5000],\n",
       "                         'tf__ngram_range': [(1, 2), (1, 2)],\n",
       "                         'tf__stop_words': ['english', None],\n",
       "                         'tf__use_idf': [True, False]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_nb = Pipeline(steps = [('tf', TfidfVectorizer()), ('nb', MultinomialNB())])\n",
    "\n",
    "pgrid_nb = {\n",
    "    'tf__max_features' : [2000, 3000, 5000],\n",
    "    'tf__stop_words' : ['english', None],\n",
    "    'tf__ngram_range' : [(1,2), (1,2)],\n",
    "    'tf__use_idf' : [True, False],\n",
    "    'nb__alpha' : [0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "gs_nb = GridSearchCV(pipe_nb,pgrid_nb,cv=5,n_jobs=-1, verbose=1)\n",
    "\n",
    "gs_nb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9004086287180462"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_nb.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes predictions on tweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245883    Anybody know if there is anyway to donate mone...\n",
       "87960     My power just went out In Katy,  tx and we hav...\n",
       "266573    There are literally people dying from #Hurrica...\n",
       "265757     #WakeUpAmerica \\n\\n#PresidentTrump is doing e...\n",
       "83590     Anyone know where they have Fiji Water? I can'...\n",
       "249344    please donate to an org that helps victims of ...\n",
       "122814    They really need to start giving these hurrica...\n",
       "61405     Weight gained during #hurricaneharvey, 2lbs an...\n",
       "63338     God bless #Texas. Texans are a resilient bunch...\n",
       "247275    But will @TheNotoriousMMA and @FloydMayweather...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_preds = gs_nb.predict(X_tweets)\n",
    "\n",
    "tweets['nb_label'] = nb_preds\n",
    "\n",
    "\n",
    "tweets[tweets['nb_label'] == 1].sample(10)['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for proper classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PLEASE HELP FIND HIM @RedCross #HurricaneHarvey #ElijahGriffin #5Yearsold #Houston '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actually needs help but no address\n",
    "tweets['tweet'][266083]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No electricity, but lots of ice, canned food and water... #HurricaneHarvey safe in Ingleside, TX'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Needs help but not urgent. Incomplete location\n",
    "tweets['tweet'][70478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've got a scared friend at 4724 Amalie St. please send help @KHOU @houstonpolice @abc13houston @HoustonTX  \""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Needs help and provides address\n",
    "tweets['tweet'][267654 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Me: *texts food and necessities we need for the weekend*\\r\\nMom: Okay but just so you know, I GOT SANGRIA SO WE'RE GOOD! #HurricaneHarvey\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not properly classified- joke\n",
    "tweets['tweet'][36323]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(nb_preds, columns = ['nb_label']).to_csv('nb_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter word counts\n",
    "\n",
    "**Proceed with caution**\n",
    "\n",
    "Creating a vocabulary dataframe will lead to a MemoryError (personal machine has 12 gb available memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(ngram_range=(1,2), stop_words='english', max_df=.9)\n",
    "cvec.fit(tweets['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cvec.vocabulary_, index= range(len(cvec.vocabulary_.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
